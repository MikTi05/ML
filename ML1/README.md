
# ML1 — Классификация (Online Shoppers Intention)

## Структура репозитория
- `research.ipynb` - ноутбук с исследованием, предобработкой и моделями
- `online_shoppers_intention.csv` - датасет (лежит в этом репозитории)

## Какой датасет выбран и почему
Выбран датасет **Online Shoppers Purchasing Intention** (`online_shoppers_intention.csv`).

Почему он:
- задача **бинарной классификации** с понятной целевой переменной `Revenue` (покупка: да/нет);
- есть **смешанные типы признаков** (числовые и категориальные), что позволяет показать предобработку (OHE + масштабирование);
- присутствует **дисбаланс классов**, что полезно для обсуждения выбора метрик (F1/Recall).

## Гипотезы
1. Из-за дисбаланса классов **Accuracy будет выше**, чем F1-score (для класса `Revenue=True`).
2. При увеличении `max_depth` у DecisionTree будет возникать **переобучение** (качество на train растёт, на test падает).
3. Признак **`PageValues`** будет одним из самых важных для предсказания `Revenue`.
4. Зависимости могут быть **нелинейными**, поэтому DecisionTree может дать **лучший F1**, чем Logistic Regression.
5. Масштабирование признаков (StandardScaler) улучшит качество **Logistic Regression** по F1.

## Краткие результаты
- **Гипотеза 1 - подтвердилась:** accuracy во всех моделях высокая, но F1 заметно ниже из-за редкого класса `Revenue=True`.
- **Гипотеза 2 - частично подтвердилась:** переобучение у дерева наблюдается, причём начинается уже на малых глубинах (примерно после 1-3), а не только после глубины 10.
- **Гипотеза 3 - подтвердилась:** `PageValues` оказался самым важным признаком по feature importance (для дерева с `max_depth=1` importance = 1.0).
- **Гипотеза 4 - подтвердилась:** DecisionTree показал более высокий F1, чем Logistic Regression, что указывает на наличие нелинейностей.
- **Гипотеза 5 - не подтвердилась:** в эксперименте Logistic Regression без масштабирования дала F1 выше, чем с масштабированием.

Итог по моделям: при выбранной основной метрике **F1** лучшая модель на тесте - **DecisionTree (GridSearch best)** (F1 = 0.64), она же даёт максимальный Recall (лучше “находит покупки”), но ценой большего числа ложноположительных.

## Ссылка на данные
https://www.kaggle.com/datasets/imakash3011/online-shoppers-purchasing-intention-dataset/data/discussion
